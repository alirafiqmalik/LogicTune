{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LogicTune: Fine-Tuning Language Models Using Formal Methods Feedback\n",
        "\n",
        "This notebook demonstrates the complete pipeline for training language models with automated verification-based feedback.\n",
        "\n",
        "**Paper Reference:** \"Fine-Tuning Language Models Using Formal Methods Feedback\"\n",
        "\n",
        "**Key Techniques:**\n",
        "- Direct Preference Optimization (DPO)\n",
        "- Formal verification for automated feedback\n",
        "- LoRA for efficient fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'logictune'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1883012093.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'src'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from logictune import (\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mbuild_traffic_intersection_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mparse_response_to_fsa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'logictune'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'src')\n",
        "\n",
        "from logictune import (\n",
        "    build_traffic_intersection_model,\n",
        "    parse_response_to_fsa,\n",
        "    score_response,\n",
        "    DPODatasetGenerator,\n",
        "    train_dpo,\n",
        "    test_trained_model,\n",
        "    compare_models,\n",
        "    evaluate_model_simple\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Build Environment Model\n",
        "\n",
        "Create a formal model of the traffic intersection environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the transition system\n",
        "system = build_traffic_intersection_model()\n",
        "\n",
        "print(f\"System states: {len(system.get_all_states())}\")\n",
        "print(f\"System transitions: {len(system.get_all_transitions())}\")\n",
        "print(f\"Atomic propositions: {system.atomic_propositions}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Test Formal Verification\n",
        "\n",
        "Demonstrate how the verification pipeline scores controller responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Safe controller response\n",
        "safe_response = \"\"\"\n",
        "1. If the light is green, go straight through the intersection.\n",
        "2. If the light is yellow, slow down and stop.\n",
        "3. If the light is red, stop and wait.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Testing SAFE controller:\")\n",
        "print(safe_response)\n",
        "\n",
        "# Parse and score\n",
        "safe_fsa = parse_response_to_fsa(safe_response, verbose=True)\n",
        "safe_score, safe_results = score_response(system, safe_fsa, verbose=True)\n",
        "\n",
        "print(f\"\\nSafe Controller Score: {safe_score}/3\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unsafe controller response\n",
        "unsafe_response = \"\"\"\n",
        "1. Always go straight regardless of the light color.\n",
        "2. Speed through yellow lights.\n",
        "3. Turn left on red lights.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Testing UNSAFE controller:\")\n",
        "print(unsafe_response)\n",
        "\n",
        "# Parse and score\n",
        "unsafe_fsa = parse_response_to_fsa(unsafe_response, verbose=True)\n",
        "unsafe_score, unsafe_results = score_response(system, unsafe_fsa, verbose=True)\n",
        "\n",
        "print(f\"\\nUnsafe Controller Score: {unsafe_score}/3\")\n",
        "print(f\"\\nScore difference: {safe_score - unsafe_score} (used for DPO training)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Generate DPO Training Dataset\n",
        "\n",
        "Generate preference pairs using formal verification as automated feedback.\n",
        "\n",
        "**Note:** This step requires GPU and downloads model weights (~4GB for TinyLlama).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize dataset generator\n",
        "generator = DPODatasetGenerator(\n",
        "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    device=\"auto\"\n",
        ")\n",
        "\n",
        "# Generate dataset\n",
        "generator.generate_dataset(\n",
        "    output_path=\"dpo_dataset.jsonl\",\n",
        "    n_responses_per_prompt=4,\n",
        "    temperature=1.0,\n",
        "    max_pairs_per_prompt=3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Train with DPO\n",
        "\n",
        "Fine-tune the model using Direct Preference Optimization with the generated dataset.\n",
        "\n",
        "**Note:** This requires GPU for efficient training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model\n",
        "train_dpo(\n",
        "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    dataset_path=\"dpo_dataset.jsonl\",\n",
        "    output_dir=\"dpo_model\",\n",
        "    num_epochs=3,\n",
        "    batch_size=2,\n",
        "    learning_rate=5e-5,\n",
        "    beta=0.1,\n",
        "    use_quantization=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Test Trained Model\n",
        "\n",
        "Generate responses from the trained model and verify them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test prompt\n",
        "test_prompt = \"Generate a step-by-step controller for safely navigating a traffic intersection with lights.\"\n",
        "\n",
        "# Generate response from trained model\n",
        "response = test_trained_model(\"dpo_model\", test_prompt)\n",
        "\n",
        "print(f\"Prompt: {test_prompt}\\n\")\n",
        "print(f\"Generated Response:\\n{response}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify the generated response\n",
        "print(\"Verifying generated response...\\n\")\n",
        "\n",
        "try:\n",
        "    controller_fsa = parse_response_to_fsa(response, verbose=True)\n",
        "    score, results = score_response(system, controller_fsa, verbose=True)\n",
        "    print(f\"\\n✓ Verification Score: {score}/3\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error during verification: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Evaluate Model Improvement\n",
        "\n",
        "Compare base model vs fine-tuned model to measure improvement in specification satisfaction rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare base model vs fine-tuned model\n",
        "comparison = compare_models(\n",
        "    base_model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    fine_tuned_model=\"dpo_model\",\n",
        "    test_prompts=None,  # Uses default test prompts\n",
        "    n_samples_per_prompt=2,  # 2 samples per prompt for faster evaluation\n",
        "    output_file=\"evaluation_results.json\"\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Base Model: {comparison['base_model']['satisfaction_rate']:.1f}%\")\n",
        "print(f\"Fine-Tuned Model: {comparison['fine_tuned_model']['satisfaction_rate']:.1f}%\")\n",
        "print(f\"Improvement: {comparison['improvement']:+.1f}%\")\n",
        "print(f\"{'='*70}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated the complete LogicTune pipeline:\n",
        "\n",
        "1. **Environment Model**: Built a formal transition system for traffic intersection\n",
        "2. **Verification**: Scored controller responses using formal methods\n",
        "3. **Dataset Generation**: Created preference pairs with automated feedback\n",
        "4. **DPO Training**: Fine-tuned model to prefer safe controllers\n",
        "5. **Evaluation**: Tested and verified the trained model\n",
        "6. **Comparison**: Measured improvement in specification satisfaction rate\n",
        "\n",
        "The key innovation is using formal verification instead of human feedback for preference learning, enabling automated and reliable fine-tuning for safety-critical control tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated the complete LogicTune pipeline:\n",
        "\n",
        "1. **Environment Model**: Built a formal transition system for traffic intersection\n",
        "2. **Verification**: Scored controller responses using formal methods\n",
        "3. **Dataset Generation**: Created preference pairs with automated feedback\n",
        "4. **DPO Training**: Fine-tuned model to prefer safe controllers\n",
        "5. **Evaluation**: Tested and verified the trained model\n",
        "\n",
        "The key innovation is using formal verification instead of human feedback for preference learning.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
