# Core dependencies for DPO-AF pipeline
# Fine-Tuning Language Models Using Formal Methods Feedback

# Deep Learning Framework
torch>=2.0.0
transformers>=4.36.0  # Uses eval_strategy (4.44+) for forward compatibility

# Training libraries
trl>=0.7.4  # Transformer Reinforcement Learning (includes DPOTrainer)
peft>=0.7.0  # Parameter-Efficient Fine-Tuning (LoRA)
accelerate>=0.25.0  # For distributed training
bitsandbytes>=0.41.0  # For quantization (optional)

# Data handling
datasets>=2.14.0
numpy>=1.24.0

# Formal Methods / Graph libraries
networkx>=3.0

# Utilities
tqdm>=4.65.0

